- 大模型的训练数据 
- 分词
  - https://github.com/huggingface/tokenizers
  - 构建一个词表，通过词表一一映射进行分词
- 模型 
- 训练
   - 预训练

```
<|fim_begin|> f_pre <|fim_hole|> f_suf <|fim_end|> f_middle <|eos_token|>
```